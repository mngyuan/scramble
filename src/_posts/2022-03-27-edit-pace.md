---
date: '2022-03-27T19:51:00.502Z'
title: Edit Pace
excerpt: Explorations in pacing of film edits
ogImage:
coverImage:
slug: 2022-03-27-edit-pace
---

# Edit Pace: Explorations in pacing of film edits

_Weekly updates and notes from the studio_

<br />

I was re-watching Tony Zhou and Taylor Ramos's [video essay on editing](https://youtu.be/3Q3eITC01Fg?t=200) when their explanation of how cut pacing translates to emotion struck a chord. _Every Frame a Painting_, their now finished video essay series, never failed to capture my imagination, but what struck me here was how data-driven their approach was - neatly breaking down the ramping up and down of shot length to get their point across. As the ship rises, the cuts get quicker and quicker, conveying excitement and Luke's growing ability with the force. Then, as the ship falls back into the swamp, the cuts take twice as long to ramp down, slowly getting longer and giving the audience time to soak in the disappointment and frustration.

![Luke Skywalker's X-Wing in the swamp](/media/blog/lukexwing.jpg)

This got me thinking - what could we learn by looking at cut lengths for other scenes and films? And what else could I learn by closely examining film cuts?

## Uncutting a film

![Drive My Car, the film](/media/blog/drivemycar-669.jpg)

<small>Drive My Car, _Hamaguchi Ryusuke's 3 hour feature adaptation of Murakami's short story_</small>

With these thoughts in mind, I watched _Drive My Car_ and decided to give myself some homework. To understand the process of editing a bit better, and to create for myself some raw material for future video editing experiments, I uncut the entire film, placing the footage into folders resembling how they'd arrive organized after principal photography ended.

First, I had to uncut the film, and settled on [pyscenedetect](https://github.com/albanie/shot-detection-benchmarks), and installed it to a new `virtualenv`.

```zsh
# Split file into scene files and save 1 image (from the middle of the scene) per file
# and generate a CSV of the scene statistics
scenedetect -i drive.my.car.2021.mkv -s drivemycarstats.csv time detect-content list-scenes save-images -n 1 split-video
```

which resulted in this:

![Folder structure of uncut film](/media/blog/folders-unorganized.jpg)

After some brief consultations with some friends in film, I decided to ... wing it with folder structure. I knew the broad strokes - which camera unit (Tokyo, Hiroshima, or Hokkaido), which sets (apartment, theatre, hotel), and which "scenarios", i.e. which actors must've been present for that day of shooting and whether it was day or night. By slowly organizing the files, I slowly got an intuition for how they were shot, and at least 4 hours (spread over 2 days) later, I ended up with this:

![Folder structure of uncut film, organized by set](/media/blog/folders-organized.jpg)

_Drive My Car_ runs 02:58:54 long, a total of 257359 frames including titles and credits, and contained roughly 741 scenes.

## Insights on _Drive My Car_

`pyscenedetect` didn't run perfectly - it seemed to struggle when two shots were extremely visually similar, i.e. in shot reverse-shot dialogue inside the car at night, where even though it's obvious to us that the face on screen has changed, it isn't clear purely by threshold analysis that anything at all has changed.

I took notes while uncutting the film, and without yet looking at the editing pacing numbers at all, I already learned some interesting things about the film **(spoiler free)**:

```markdown
## TOKYO

- The only profile shots of driving occur before something bad is about to happen
- A mirror in moments of importance

## HIROSHIMA

- The only non-car establishing shots are of the fountain

## HOKKAIDO

- The movie's first lateral tracking shot of the car appears at the beginning here
- Also introduces the moving establishing shot shot from the car, not crane or helicopter

## KOREA

- The movie ends on this new car-less establishing shot
```

It's super interesting postulating on what these changes in visual language mean for its narrative story telling. For example, the addition of the location establishing shot from _within_ the car in the latter third of the film gives us a feeling of being in the car, and helps us feel the comforting ups and downs of its vibrations in a way none of the other stabilized or crane shots do.

<details>
<summary>Spoilers</summary>
Or the fact that Kafuku's crash is the first full profile shot in the car, the angle which focuses the most on the person and reveals the least about where they are headed, but profile driving shots start to pepper the movie from that point forward. The younger actors crash, by comparison, plays out in a much more comfortable 3/4 perspective, and far away, from Kafuku-san's car.
</details>

## Building a visualization

![One frame from every scene, 741 in total, of Drive My Car](/media/blog/drivemycar-every-scene.jpg)

<small>_One frame from every scene, 741 in total_</small>

Using the images of each scene, I want to reconstruct a timeline of the scenes in the film, one where you can zoom, seek, and hover for more information on the current shot (length, maybe narrative function). But I ran into the classic stumbling block I've been hitting against for many years now: coding is _hard_.

Like, not impossibly hard, and by many accounts I'm not bad at it, but undoing the years of being trained to write what I call "corporate" code - easily maintained, self documenting, modular, cutting edge - to learn how to code improvisationally - repeating myself, YAGNI, KISS - has been difficult to say the least. The most important change in my life has been that code has transformed from an end - the main output that I create - to a tool - something used alongside other tools towards a different end. How do you go, for example, from fretting over every fine detail of an artisanal table to prototyping and designing an interior set?

## Help: Fast video seeking

I've gotten a bit stumped, thinking about which drawing libraries might support zooming and seeking by dragging better, and how to quickly (<32ms delay) seek to different to different parts of 3 hours of footage. If anyone has suggestions on how to quickly jump to an arbitrary point within 3 hours of video, please let me know at [me@mngyuan.com](mailto:me@mngyuan.com).

Part of the reason I need a quick response time is the tactility of the interface. It's intended to be used physically, with input from sliders and knobs, etc, and from testing [my interactive record player project](https://blog.bela.io/here-and-now/) I know that 150ms, roughly the delay on Arduino, is unacceptably high for immediacy, and 16ms, the latency of [Bela](https://learn.bela.io/using-bela/about-bela/understanding-real-time/), feels much better. From playing Melee I feel that discerning users won't accept more than 4 frames of input lag on a 60fps game, meaning 66ms at most.

## Next week

I've sculpted out a light skeleton of what I might be up to next week:

```markdown
[ ] reach out to connections for workshop participants
[ ] finish a seeking prototype for choosing selects
[ ] design an editing workshop using this prototype
[ ] talk to someone who has experience running workshops
[ ] uncut another film to have comparison material?
[ ] purchase components
```

It's been very exciting and energizing learning through prototyping, and as I mentioned before, it's a skill I'm still getting used to (feels like it's a bike I'm still learning to ride). But getting that visualization of all the scenes together really felt like it paid off, especially knowing in the future I could automate it for other movies.

The code for the experiments etc. from this week can be found on Github [here](https://github.com/mngyuan/video-editing-experiments/tree/main/scene-cut-display).

![View from my desk](/media/blog/IMG_5217.jpeg)
<small>_The view from my desk_</small>
